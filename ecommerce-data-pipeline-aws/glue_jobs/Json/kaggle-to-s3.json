{
	"jobConfig": {
		"name": "kaggle-to-s3",
		"description": "",
		"role": "arn:aws:iam::822687513164:role/glue-role-ecommerce-pipeline",
		"command": "glueetl",
		"version": "3.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "kaggle-to-s3.py",
		"scriptLocation": "s3://aws-glue-assets-822687513164-eu-north-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-14T05:24:49.951Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-822687513164-eu-north-1/temporary/",
		"additionalPythonModules": "kaggle,boto3,tqdm",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-822687513164-eu-north-1/sparkHistoryLogs/",
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# ──────────────────────────────────────────────────────────────\n#  Glue Job : kaggle_to_s3_full.py\n#  Author   : Milan Gabriel\n#  Created  : 2025-07-14\n#  Purpose  : Download Kaggle e-commerce clickstream dataset\n#             and upload each CSV to S3 raw zone.\n# ──────────────────────────────────────────────────────────────\n\n# 0️⃣  Glue boilerplate (must be first)\nimport sys, os, zipfile\nfrom pathlib import Path\n\nfrom awsglue.context import GlueContext\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.job import Job\nfrom pyspark.context import SparkContext\n\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\nsc   = SparkContext()\nglue = GlueContext(sc)\nspark = glue.spark_session\njob  = Job(glue)\njob.init(args[\"JOB_NAME\"], args)\n\n# 1️⃣  Fix permissions & env for Kaggle API\nos.environ[\"HOME\"] = \"/tmp\"                               # Writable HOME\nKAGGLE_USERNAME     = \"milangabriel\"                      # ← replace\nKAGGLE_KEY          = \"cbcd5766f1f04662ace25b55231a8d90\"  # ← replace\n\nkaggle_dir = Path(\"/tmp/.kaggle\")\nkaggle_dir.mkdir(parents=True, exist_ok=True)\nwith open(kaggle_dir / \"kaggle.json\", \"w\") as f:\n    f.write(f'{{\"username\":\"{KAGGLE_USERNAME}\",\"key\":\"{KAGGLE_KEY}\"}}')\nos.chmod(kaggle_dir / \"kaggle.json\", 0o600)\n\n# 2️⃣  Import extra libraries (available because of --additional-python-modules)\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nfrom tqdm import tqdm\nimport boto3\n\n# 3️⃣  Download dataset to /tmp\napi = KaggleApi(); api.authenticate()\nDATASET   = \"mkechinov/ecommerce-behavior-data-from-multi-category-store\"\nLOCAL_DIR = Path(\"/tmp/ecom_raw\")\nLOCAL_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"⬇️  Downloading Kaggle dataset …\")\napi.dataset_download_files(DATASET, path=LOCAL_DIR, unzip=True)\nprint(\"✅  Download done\")\n\n# 4️⃣  Upload each CSV to S3 raw zone\ns3     = boto3.client(\"s3\")\nbucket = \"ecommerce-pipeline-milan\"\nprefix = \"raw/clickstream/\"        # trailing slash important\n\ncsv_files = list(LOCAL_DIR.glob(\"*.csv\"))\nprint(f\"⬆️  Uploading {len(csv_files)} CSV files to s3://{bucket}/{prefix}\")\n\nfor csv_path in tqdm(csv_files):\n    s3.upload_file(\n        Filename=str(csv_path),\n        Bucket=bucket,\n        Key=f\"{prefix}{csv_path.name}\"\n    )\n\nprint(\"🎉  All CSVs uploaded to S3\")\n\n# 5️⃣  Clean up (optional)\n# shutil.rmtree(LOCAL_DIR)  # uncomment if you want to free /tmp space\n\n# 6️⃣  Finish\njob.commit()"
}