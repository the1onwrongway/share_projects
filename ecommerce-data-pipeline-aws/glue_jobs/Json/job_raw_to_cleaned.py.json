{
	"jobConfig": {
		"name": "job_raw_to_cleaned.py",
		"description": "",
		"role": "arn:aws:iam::822687513164:role/glue-role-ecommerce-pipeline",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "job_raw_to_cleaned.py",
		"scriptLocation": "s3://aws-glue-assets-822687513164-eu-north-1/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-13T13:18:43.268Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-822687513164-eu-north-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-822687513164-eu-north-1/sparkHistoryLogs/",
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "\"\"\"\nGlue Job – Raw ➜ Cleaned Clickstream  (author: Milan Gabriel)\n\"\"\"\n\nfrom awsglue.context import GlueContext\nfrom awsglue.job     import Job\nfrom awsglue.utils   import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import col, to_timestamp, regexp_replace, year, month\nimport sys\n\nargs = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n\nsc     = SparkContext()\nglue   = GlueContext(sc)\nspark  = glue.spark_session\njob    = Job(glue)\njob.init(args[\"JOB_NAME\"], args)\n\n# 1. Read & narrow columns early\nraw_df = (\n    spark.read.option(\"header\", \"true\")\n         .csv(\"s3://ecommerce-pipeline-milan/raw/clickstream/\")\n         .select(\"event_time\", \"event_type\", \"product_id\",\n                 \"category_id\", \"category_code\", \"brand\",\n                 \"price\", \"user_id\", \"user_session\")\n)\n\n# 2. Transform\ncleaned_df = (\n    raw_df\n      .filter(col(\"event_type\") == \"purchase\")\n      .withColumn(\"event_time\",\n                  to_timestamp(regexp_replace(\"event_time\", \" UTC\", \"\"),\n                               \"yyyy-MM-dd HH:mm:ss\"))\n      .withColumn(\"product_id\",  col(\"product_id\").cast(\"bigint\"))\n      .withColumn(\"category_id\", col(\"category_id\").cast(\"bigint\"))\n      .withColumn(\"price\",       col(\"price\").cast(\"double\"))\n      .withColumn(\"user_id\",     col(\"user_id\").cast(\"bigint\"))\n      .withColumn(\"year\", year(\"event_time\"))\n      .withColumn(\"month\", month(\"event_time\"))\n      .drop(\"event_type\")              # no longer needed\n)\n\n# 3. Write partitioned Parquet\n(\n    cleaned_df\n      .write\n      .mode(\"overwrite\")               # change to \"append\" for incremental loads\n      .option(\"compression\", \"snappy\")\n      .partitionBy(\"year\", \"month\")\n      .parquet(\"s3://ecommerce-pipeline-milan/clean/clickstream/purchases/\")\n)\n\njob.commit()"
}