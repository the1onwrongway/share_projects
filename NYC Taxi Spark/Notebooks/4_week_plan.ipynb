{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d88f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üóìÔ∏è 4-Week Spark + Databricks Project Plan (NYC Taxi)\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ Week 1 ‚Äì Finalize Core Pipeline ‚úÖ (Completed)\n",
    "\n",
    "### ‚úÖ Already doing:\n",
    "- [x] Ingestion from Parquet\n",
    "- [x] Cleaning\n",
    "- [x] Feature engineering\n",
    "- [x] Aggregation\n",
    "\n",
    "### üìå To finish:\n",
    "- [x] Save raw + enriched data in Delta format\n",
    "- [x] Clean notebook structure\n",
    "- [x] Add markdown comments\n",
    "\n",
    "### üß† Concepts covered:\n",
    "- [x] Delta format\n",
    "- [x] Filtering + deriving columns in Spark\n",
    "- [x] Spark transformations\n",
    "- [x] (Optional) Partitioning and schema evolution ‚Äî planned for later\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ Week 2 ‚Äì Query & Visualize üü° (In Progress)\n",
    "\n",
    "### üí° Learn Databricks SQL\n",
    "- [ ] Create a dashboard in Databricks SQL or notebook SQL\n",
    "\n",
    "### üìä Create dashboard:\n",
    "- [ ] Hourly tip percent by day\n",
    "- [ ] Top pickup locations\n",
    "\n",
    "### üõ†Ô∏è Optimize:\n",
    "- [ ] Repartition or cache transformed tables\n",
    "- [ ] Use `.explain()` and `.count()` for performance profiling\n",
    "\n",
    "### ‚è±Ô∏è Time Estimate:\n",
    "~8‚Äì10 hours total\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ Week 3 ‚Äì AWS Foundations ‚è≥ (Planned)\n",
    "\n",
    "### üéì Learn:\n",
    "- [ ] IAM basics: users, roles, access keys\n",
    "- [ ] S3: create buckets, upload, set permissions\n",
    "\n",
    "### üîå Connect Databricks to S3:\n",
    "- [ ] Use `spark.conf.set(...)` with `s3a://` path\n",
    "- [ ] Test reading/writing `.parquet` files to/from S3\n",
    "\n",
    "### üß∞ Resources:\n",
    "- [ ] AWS Free Tier account setup\n",
    "- [ ] (Optional) Boto3 intro for scripting\n",
    "\n",
    "### ‚è±Ô∏è Time Estimate:\n",
    "~9‚Äì12 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üîµ Week 4 ‚Äì Automate + Polish ‚è≥ (Planned)\n",
    "\n",
    "### ‚öôÔ∏è Automate pipeline:\n",
    "- [ ] Use Databricks Jobs (via UI or JSON)\n",
    "- [ ] Schedule notebook to run daily/hourly\n",
    "\n",
    "### üßæ GitHub polish:\n",
    "- [ ] Add `README.md` (overview, tools used, how to run)\n",
    "- [ ] Push notebook as `.dbc` or `.ipynb`\n",
    "\n",
    "### üíº Resume Line (sample):\n",
    "> ‚ÄúBuilt an end-to-end data pipeline using Spark + Delta on Databricks, processed NYC Taxi data, and automated data ingestion with AWS S3.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2fd23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
